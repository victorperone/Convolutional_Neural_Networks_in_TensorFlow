# Module 2 Augmentation A Technique to Avoid Overfitting

> **Fixing the "Lumberjack" Problem:** Using Data Augmentation and Dropout to build a generalized, production-ready model.

[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.0+-orange.svg)](https://www.tensorflow.org/) [![Keras](https://img.shields.io/badge/Keras-Image_Data_Generator-red.svg)](https://keras.io/preprocessing/image/) [![Python](https://img.shields.io/badge/Python-3.x-blue.svg)](https://www.python.org/)

---

## üß≠ Overview

In Module 1, we successfully built a pipeline for the **Cats vs. Dogs** dataset, but we hit a wall: **Overfitting**. Our model achieved 99% accuracy on training data but failed to generalize to new images (stuck at ~70% validation accuracy). It memorized the training images instead of learning the features.

In **Module 2**, we solve this using **Data Augmentation**.

Instead of feeding the model the exact same image every epoch, we mathematically distort the images on the fly‚Äîrotating, zooming, shifting, and shearing them. This forces the model to learn that a "cat" is still a cat, even if it is upside down or zoomed in.

This module introduces:
* **ImageDataGenerator Augmentation:** Transforming images in memory during training.
* **Dropout Regularization:** Randomly disabling neurons to prevent reliance on specific features.
* **Stable Training:** Closing the gap between Training and Validation accuracy.

---

## üéØ Learning Objectives

In this module, you will learn to:

* **Implement Data Augmentation:** Configure `ImageDataGenerator` to perform random rotations, shifts, shears, zooms, and horizontal flips.
* **Apply Dropout:** Add `Dropout` layers to your neural network to randomly "turn off" neurons during training, forcing the network to learn redundant, robust features.
* **Evaluate Robustness:** Analyze how augmentation prevents the "exploding loss" curve we saw in Module 1.
* **Balance Underfitting vs. Overfitting:** Understand that augmentation makes the training task *harder*, so training accuracy might go down, but validation accuracy (real-world performance) goes up.

---

## üíº Why This Project Matters

This module demonstrates my ability to:

- Diagnose overfitting using training/validation curves
- Apply regularization techniques (Dropout)
- Use data augmentation to improve generalization
- Build reproducible deep learning pipelines
- Interpret model behavior beyond accuracy scores

---

## üë• Who This Module Is For

This module is designed for:
- **Developers facing Overfitting**: If your model has 99% accuracy in training but fails in the real world, this is the fix.
- **Engineers with Small Datasets**: Learn how to artificially expand a dataset of 2,000 images into an effectively infinite number of variations.
- **Computer Vision Students**: Understand the trade-off between Training Speed and Model Robustness.

---

## üõ†Ô∏è Skills Demonstrated

### 1. Advanced Data Augmentation (The "Infinite" Dataset)

In Module 1, we had a static dataset. If the model saw a cat facing left, it learned "cat facing left." If it saw a cat facing right, it might think it's a different animal.

**The Solution**: We use `ImageDataGenerator` to dynamically generate new training samples on the fly. 
This effectively turns our 2,000 images into an infinite stream of variations, preventing the model from memorizing specific pixel patterns.

```python
train_datagen = ImageDataGenerator(
    rescale=1./255,         # Normalize pixel values to [0,1]
    rotation_range=40,      # Rotate the image randomly up to 40 degrees
    width_shift_range=0.2,  # Shift the image horizontally by up to 20%
    height_shift_range=0.2, # Shift the image vertically by up to 20%
    shear_range=0.2,        # Shear transformation (slanting the image)
    zoom_range=0.2,         # Randomly zoom inside the image by up to 20%
    horizontal_flip=True,   # Randomly flip the image horizontally
    fill_mode='nearest'     # Strategy for filling in newly created pixels
)
```

#### Technical Breakdown:

- `rotation_range=40`: The generator will randomly rotate the image between -40 and +40 degrees. This teaches the model that a "tilted cat" is still a cat (invariance to orientation).
- `width_shift_range=0.2` & `height_shift_range=0.2`: Moves the subject off-center. This forces the Convolutional layers to learn features (ears, tails) regardless of their x,y position in the frame (Translation Invariance).
- `shear_range=0.2`: Applies a "slant" transformation, mimicking the effect of looking at an object from a different angle (perspective distortion).
- `zoom_range=0.2`: Randomly zooms in. This simulates the subject being closer or further away, preventing the model from relying on size as a feature.
- `horizontal_flip=True`: Doubles the diversity of the dataset. A dog looking left is structurally the same as a dog looking right.
- `fill_mode='nearest'`: When we rotate or shift an image, empty black space is created. This parameter tells Keras to fill that space by copying the color of the nearest valid pixel, avoiding sharp black edges that could confuse the model's edge detectors.

### 2. Dropout Regularization (The "Anti-Memorization" Layer)

This is arguably the most important concept introduced in this module. We add a Dropout layer before the final Dense layer.

We add a new type of layer to our model architecture.

```python
model = tf.keras.models.Sequential([
    # ... Convolutional Layers ...
    tf.keras.layers.Flatten(),
    
    # The Dropout Layer
    tf.keras.layers.Dropout(0.2), 
    
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```
**Why?**
- Imagine a team of employees where one person knows everything. If they get sick, the team fails. Dropout fires 20% of the team randomly every day. This forces everyone to learn the job, making the whole team (the model) more robust.

**What is Dropout?** 
- Dropout is a regularization technique where, during training, the network randomly "drops" (sets to zero) a percentage of neurons in the layer. Here, **0.2 means 20% of the neurons are turned off randomly for every single training step**.

Why is this a "Superpower" for the model?

**1. Prevents "Grandmother Cells"**: Without dropout, neurons can "co-adapt." One neuron might learn to fix the mistakes of another, leading to a fragile chain where if one feature is missing (e.g., the ear is hidden), the whole prediction fails.

**2. Forces Redundancy**: By killing 20% of the neurons, the network cannot rely on any single feature (like "pointy ears") to identify a cat. It is forced to learn multiple ways to identify a cat (whiskers, tail, fur texture) because it never knows which neurons will be active.

**3. The Ensemble Effect**: You can think of Dropout as training thousands of different "thinner" neural networks and averaging them together. Ensembles almost always outperform single models.

**The "Committee" Analogy**: Imagine a committee making decisions. If one person is the loud expert and everyone else just agrees with them, the committee is weak. If the expert gets sick, the committee fails. **Dropout** is like randomly banning 50% of the committee members from every meeting. This forces everyone on the committee to learn the material and become an expert, making the final decision much more robust.

### 3. Monitoring "Exploding Loss"

A critical skill demonstrated here is interpreting the loss curves to verify that Augmentation is working.

- **Module 1 (No Augmentation)**: Validation Loss goes UP (Explodes) as the model gets overconfident.
- **Module 2 (With Augmentation)**: Validation Loss stays **FLAT** or goes **DOWN**.
  - Note: The training accuracy will likely be **lower** in this module (e.g., 85% instead of 99%). 
  - **This is good!** It means the model is struggling to learn the harder, augmented data, which prevents it from memorizing. The gap between Training and Validation narrows, representing a true "Generalized" model.


---

## ‚ö†Ô∏è Common Mistakes Explored in This Module

- **Augmenting Validation Data**: You should **NEVER** augment your validation/test data. You only want to make the training hard. The validation set should represent real, unaltered images. 
  - **Correct**: `validation_datagen = ImageDataGenerator(rescale=1./255)` (Rescale only!)
- **Too Much Augmentation**: If you rotate a "6" by 180 degrees, it becomes a "9". Augmentation must make sense for your data. For cats/dogs, vertical flips (upside down) might be confusing if photos are always upright.
- **Training Time**: Augmented training takes longer to converge because the model sees "different" images every epoch. You often need more epochs (e.g., 100 vs 15).

---

## ‚ñ∂Ô∏è How to Run

This module consists of Jupyter notebooks that can be run locally or on Google Colab.

### Prerequisites

- Python 3.8 or higher
- pip
- Virtual environment support (recommended)

### 1. Clone the Repository

```bash
git clone https://github.com/victorperone/Convolutional_Neural_Networks_in_TensorFlow.git
cd Module2_Augmentation-A_Technique_to_Avoid_Overfitting
```

### 2. Create and Activate a Virtual Environment (Recommended)

**Linux / macOS**
```bash
python3 -m venv venv
source venv/bin/activate
```

**Windows**
```bash
python -m venv venv
venv\Scripts\activate
```

### 3. Install Dependencies

All required packages are listed in `requirements.txt`

```python
pip install -r requirements.txt
```
This will install TensorFlow and other necessary libraries.

### 4. Launch Jupyter Notebook

You can run the notebooks **locally** or using **Google Colab**.

#### Option A: Run Locally (Jupyter Notebook)

```bash
jupyter notebook
```

or, if you prefer JupyterLab:
```bash
jupyter lab
```

### Option B: Run on Google Colab (No Local Setup Required)

1. Go to: [Google Colab](https://colab.research.google.com)
2. Click File ‚Üí Open notebook
3. Select the GitHub tab
4. Paste your repository URL
5. Open  `File.ipynb`

Google Colab provides:

- Free CPU (and optional GPU) execution
- No local Python or TensorFlow installation
- Automatic dependency handling for most libraries

‚ö†Ô∏è Note: If requirements.txt is not automatically handled, install dependencies in a Colab cell:

```python
!pip install -r requirements.txt
```

### 5. Run the Exercises

Open the notebooks in numerical order
Run each cell sequentially
Observe how changes in model architecture, training duration, and callbacks affect results
It is recommended to run the exercises **in order**, as each one builds conceptually on the previous examples.

### Environment Notes

- TensorFlow may produce informational or warning messages during execution.
- These messages do not affect the correctness of the exercises.
- CPU execution is sufficient for all notebooks in this module, though GPU is recommended for faster training on the 300x300 images.

---

## üß™ Reproducibility Note

Model training involves random initialization of weights.
As a result:
- Exact accuracy and loss values may vary slightly between runs
- Overall trends and conclusions should remain consistent

---

## ‚ùì Problem Statement

In Module 1, we built a powerful Convolutional Neural Network that achieved nearly 100% accuracy on the training set. However, it suffered from severe `Overfitting`‚Äîit memorized the specific pixels of the training images (e.g., "all cats are on rugs") rather than learning the general features of a cat.

**The Challenge**: How do we force the model to learn invariant features (ears, whiskers, tails) that work regardless of the animal's position, size, or orientation?

**The Solution**: We introduce Data Augmentation to distort images during training and Dropout to prevent neuron co-adaptation, creating a model that generalizes to unseen data.

> *Note: For this module, we use a filtered subset provided by Microsoft Research to ensure compatibility with free Colab instances.*

---

## üíæ Dataset
We utilize the famous **Kaggle Cats vs. Dogs Dataset**.
* **Size:** 25,000 Color Images (RGB).
* **Classes:** Binary (Cat = 0, Dog = 1).
* **Characteristics:** Unlike MNIST (which is 28x28 grayscale), these images are:
    * **High Resolution:** Up to 500x500 pixels.
    * **Varied Scenes:** Indoor, outdoor, different lighting, different angles.
    * **Varied Subjects:** Different breeds, colors, and poses.
* **Augmentation Twist**: Unlike Module 1, the model never sees the same image twice.
  - Every time an image is loaded, `ImageDataGenerator` applies random transformations (rotation, zoom, shear).
  - This effectively turns our static dataset into an **infinite stream of unique variations**.

If desired you can access full dataset [Cats vs Dogs on Kaggle](https://www.kaggle.com/competitions/dogs-vs-cats).

> *Note: For this module, we use a filtered subset provided by Microsoft Research to ensure compatibility with free Colab instances.*

---

## üìâ Deep Dive: The Solution to Overfitting

In Module 1, our graphs showed **Training Loss** going down and **Validation Loss** exploding up.

In this module, by making the training data harder (augmented), we see a different story:

1. **Training Accuracy** is lower (maybe 85% instead of 99%).
2. **Validation Accuracy** is higher (climbing to 80%+).
3. **The Curves Converge**: The Training and Validation lines move together. This means the model is actually learning, not memorizing.

- **Key Takeaway**: Augmentation reduces the "variance" of the model. We sacrifice a bit of training speed/accuracy for a massive gain in generalization.

---

### ‚öôÔ∏è Technical Implementation

### 1. Data Preprocessing (`ImageDataGenerator`)
Because real-world datasets are too large for RAM, we use generators to stream data.

```python
train_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    '/tmp/cats-v-dogs/training/',
    target_size=(150, 150),  # Resizes images to 150x150
    batch_size=128,
    class_mode='binary'
)
```

### 2. Model Architecture

We use a stack of Conv2D and MaxPooling layers.

```python
model = tf.keras.models.Sequential([
    # Input: 150x150 with 3 Color Channels
    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    
    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    
    # Sigmoid Output: Returns value 0-1 (Cat vs Dog)
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```

#### Convolutional Neural Networks (CNNs)

CNNs preserve spatial structure and learn visual features such as edges and shapes.

- **Intuition:** Dense networks see pixels; CNNs see patterns.

#### Layer Explanation

- **Conv2D (Convolutional Layers with ReLU Activation)**: Convolutional layers are the core building blocks of CNNs. Instead of connecting every pixel to every neuron, they operate on small local regions of the image.
  - Applies multiple learnable 3√ó3 filters (kernels) that slide across the image
  - Each filter performs a dot product between its weights and a small patch of the image
  - The result of each filter is a feature map highlighting where a specific pattern appears
  - Early convolutional layers typically learn simple features:
    - Edges (horizontal, vertical, diagonal)
    - Corners
    - Basic textures
  - Deeper convolutional layers combine earlier features to detect more complex patterns:
    - Shapes
    - Object parts
    - High-level visual structures
  - Weight sharing means the same filter is applied across the entire image:
    - Greatly reduces the number of parameters
    - Makes the model efficient and scalable
  - ReLU activation:
    - Keeps positive feature responses
    - Removes negative values
    - Introduces non-linearity so the model can learn complex patterns
  - **üîë Key intuition:**
    - Convolutional layers learn what to look for and where it appears in the image.

- **MaxPooling2D**: Pooling layers reduce the size of feature maps while retaining the most important information.
  - Operates on small windows (commonly 2√ó2) of each feature map
  - Replaces each window with its maximum value
  - Reduces spatial dimensions (width and height) by a factor of 2
  - Keeps the strongest feature activations while discarding weaker ones
  - Makes the network:
    - Faster (fewer computations)
    - Less memory-intensive
    - More resistant to small shifts or distortions in the image
  - Helps prevent the network from focusing too much on exact pixel locations
  - **üîë Key intuition:**
    - Pooling answers whether a feature is present, not exactly where it is.

- **Flatten**
  - Converts the 2D feature maps into a 1D vector
  - Prepares convolutional features for the dense layers
- **Dense (fully connected layer with ReLU activation)**
  - Learns higher-level combinations of extracted features
  - Acts as a classifier based on convolutional features
- **Dense (1 neuron, Sigmoid output)**
  - Outputs a single probability score between 0 and 1.
  - Used specifically for binary classification.
  - **Thresholding:** Typically, values < 0.5 are predicted as Class 0, and values >= 0.5 are Class 1.

The number of convolutional filters and dense neurons is varied across experiments to demonstrate how **model capacity** affects feature learning and classification performance.

---

## üß† Model Compilation

For binary classification, the optimizer and loss function change to reflect the nature of the output.

### Optimizer: RMSprop
While Adam is common, RMSprop is often preferred for automating the learning rate adjustment in recurrent or deep convolutional networks.

### Loss Function: Binary Crossentropy
Since we have only two classes and a single output neuron (0 to 1), we use binary_crossentropy instead of sparse_categorical_crossentropy.

```python
from tensorflow.keras.optimizers import RMSprop

model.compile(loss='binary_crossentropy',
              optimizer=RMSprop(learning_rate=0.001),
              metrics=['accuracy'])
```

---

## üèãÔ∏è Training the Model

### Training with Generators

In this module, training uses a **Python Generator**, differing from the standard model.fit(x, y) approach used in Course 1. This allows for **memory-efficient** streaming of data.


Since data is being streamed, we use the fit method with the generator object.

The `fit` Method with Generators
Because our dataset (25,000 images) is too large to fit into RAM, we use the train_generator to stream images in batches.

```python
history = model.fit(
    train_generator,
    epochs=100, # Increased epochs because augmentation makes learning harder!
    verbose=1,
    validation_data=validation_generator
)
```

**Key Parameters Explained**
- `train_generator`: The source of the training images (yields batches of 128 images).
- `validation_data`: The source of the validation images. The model evaluates itself against this data at the end of every epoch.
- `steps_per_epoch` (_Implicit_): In modern TensorFlow, if you don't specify this, it defaults to `len(generator)`, which is `Total Images / Batch Size`.

**Note on Performance:** You will notice that training is slower than with MNIST. This is because we are processing color images (3 channels) that are 150x150 pixels (22,500 inputs per channel), compared to the tiny 28x28 grayscale images from the previous course.

---

## üìâ Model Evaluation

This measures how well the model performs on **unseen data**.

Evaluating model performance in this module goes beyond just looking at the final accuracy score. We focus on two key aspects:
Evaluation in this module is focused on detecting Overfitting. We do not just look at the final "Test Score"; we analyze the history of training.

- **Training vs. Validation Accuracy:** We plot the accuracy history to check for overfitting.
  - **Good Model:** Training and Validation accuracy increase together.
  - **Overfitting:** Training accuracy hits 99-100%, but Validation accuracy stalls or decreases (e.g., remains at 80%).
- **Visualizing Intermediate Representations:** We use specific code blocks to visualize the "internal state" of the Convolutional layers. This allows us to see exactly what features (lines, shapes, textures) the model is activating on.

```python
# Example of accessing history for evaluation
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
```

---

## üìä Model Evaluation Metrics

In this module, we track two primary metrics. It is crucial to understand why they behave differently. 

1. Accuracy (`accuracy` & `val_accuracy`)

- **Definition**: The percentage of images classified correctly.
- **Goal**: Maximize `val_accuracy` (closer to 1.0).
- **Observation**: Training accuracy converges to 1.0 (100%), while validation accuracy plateaus. This gap represents the model's inability to generalize.

2. Loss (`loss` & `val_loss`)

- **Definition**: A penalty score calculated by **Binary Crossentropy**. It punishes the model for being "confident and wrong."
- **Goal**: Minimize this (closer to 0.0).
- **Observation**:
  - If the model predicts "Dog" (1.0) and it is a Dog: **Low Loss**.
  - If the model predicts "Dog" (1.0) and it is a Cat: **Massive Loss**.

---

## üìè Metrics in This Module

We configure the model to track these specific metrics during compilation:

```python
from tensorflow.keras.optimizers import RMSprop

model.compile(
    optimizer=RMSprop(learning_rate=0.001),
    loss='binary_crossentropy', # Mandatory for 2-class problems
    metrics=['accuracy']        # We only need accuracy for classification
)
```

- **Optimizer**: We use **RMSprop** instead of Adam. RMSprop is often preferred for Recurrent Neural Networks (RNNs) but is also excellent for Convolutional networks as it allows for an adaptive learning rate, which helps navigate the complex "loss landscape" of image data.
- **Loss**: We use **Binary Crossentropy** because our final layer is a single neuron (Dense(1, activation='sigmoid')). It treats the output as a probability: P(Class=1).


---

## üîë Key Concepts

### 1. The "Lumberjack vs. Sailor" Problem (Overfitting)

As explained in the lessons, overfitting is like a human trying to classify "Lumberjacks" vs. "Sailors."
* If all the lumberjacks in your training photos happen to be wearing caps, and no sailors are...
* The model might conclude: **"Wearing a Cap = Lumberjack."**
* This is **Overfitting**: learning a coincidental feature (the cap) rather than the essential feature (the uniform or context). In this module, we see our model doing exactly this‚Äîgetting 99% accuracy on training data but failing on the validation set.

### 2. Preprocessing & Generators
Since we cannot load 25,000 images into RAM at once, we use `ImageDataGenerator`. This tool acts as a streaming service, pulling images from the hard drive, resizing them on the fly, and feeding them to the GPU in batches.

### Model Capacity

Overfitting occurs when the model learns the training data too well but fails to generalize. Common causes explored in this module:

- Too many neurons or layers (Excessive Capacity)
- Training for too many epochs

Later exercises introduce techniques to control training behavior instead of blindly increasing epochs.

### Overfitting

Overfitting occurs when:

- The model learns the training data too well
- It fails to generalize to unseen data

Common causes explored in this module:

- Too many neurons
- Too many layers
- Too many training epochs

Later exercises introduce techniques to control training behavior instead of blindly increasing epochs.

---

## üìä Results

| Model | Train Accuracy | Validation Accuracy | Gap |
|-------|----------------|--------------------|------|
| Without Augmentation | 99% | 70% | 29% |
| With Augmentation + Dropout | 85% | 82% | 3% |

---

## üí° What I learned

- Overfitting is not solved by adding more layers ‚Äî it is solved by improving generalization.
- Data augmentation increases bias slightly but dramatically reduces variance.
- Lower training accuracy can indicate a healthier model.
- Regularization techniques are essential before scaling model capacity.

---

## üìì Notebooks & Exercises

A guided lesson demonstrating how to fix the overfitting problem from Module 1 using `ImageDataGenerator`.
- **Highlight**: Visualizing how the validation loss curve flattens and converges with the training loss, proving the model is now generalizing instead of memorizing.
**The Assignment**: Modify the previous pipeline to include robust data augmentation and regularization.
- **Task 1**: Configure `ImageDataGenerator` with rotation, shifting, shearing, and zooming parameters.
- **Task 2**: Add a `Dropout(0.5)` layer to the model architecture to prevent neuron co-adaptation.
- **Task 3**: Train the model for more epochs (since the task is harder) and analyze the improved stability in the accuracy graphs.

---

## üìù Summary

By adding **Data Augmentation** and **Dropout**, we successfully fixed the overfitting problem from Module 1.
- **Before** (Module 1): Training Acc: 100%, Validation Acc: ~70% (Gap: 30%)
- **After** (Module 2): Training Acc: ~85%, Validation Acc: ~80%+ (Gap: 5%)

The model is now robust and ready for real-world testing. However, to get even higher accuracy (95%+), we would need Transfer Learning, which is the topic of the next course module!

---

## üìò Files in This Module

```
üìÅ Module2_Augmentation-A_Technique_to_Avoid_Overfitting
‚îú‚îÄ‚îÄ üìì Course_2_Part_4_Lesson_2_Notebook_(Cats_v_Dogs_Augmentation).ipynb
‚îú‚îÄ‚îÄ üìì Course_2_Part_4_Lesson_4_Notebook.ipynb
‚îú‚îÄ‚îÄ üìì Exercise_2_Cats_vs_Dogs_using_augmentation_Question-FINAL.ipynb
‚îú‚îÄ‚îÄ üìÑ requirements.txt
‚îî‚îÄ‚îÄ üìÑ README.md
```

---

## üõë Limitations

While Augmentation and Dropout are powerful, they come with trade-offs explored in this module:
- **Slower Convergence**: Because the data is constantly changing, the model takes longer to learn. You will need more epochs (e.g., 30-100) compared to Module 1.
- **Lower "Training" Accuracy**: You will likely **never** reach 100% training accuracy again. This is expected! The task is now much harder, but the result is a better, more honest model.
- **Computationally Expensive**: Augmenting images on the fly adds CPU overhead to the training pipeline.
- **Not a Magic Bullet**: If the original dataset is too small or non-representative, augmentation alone cannot fix it. For that, we need **Transfer Learning** (Module 3).

---

## üìö Further Reading

- [Kaggle Cats vs Dogs Dataset](https://www.kaggle.com/c/dogs-vs-cats)
- [Keras Data Augmentation Documentation](https://keras.io/api/data_loading/image/)
- [Dropout: A Simple Way to Prevent Neural Networks from Overfitting (Original Paper)](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)
- [Visualizing Data Augmentation](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/)
